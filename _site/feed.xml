<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-06-27T17:41:52+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">André Fernandes</title><subtitle>My curriculum vitae</subtitle><entry><title type="html">Online course about Machine Learning with Python</title><link href="http://localhost:4000/udemy-ml-ds-dl-python/" rel="alternate" type="text/html" title="Online course about Machine Learning with Python" /><published>2021-06-26T00:00:00+01:00</published><updated>2021-06-26T00:00:00+01:00</updated><id>http://localhost:4000/udemy-ml-dl-python</id><content type="html" xml:base="http://localhost:4000/udemy-ml-ds-dl-python/">&lt;ul&gt;
  &lt;li&gt;Recently, I finished an online course on &lt;strong&gt;Udemy&lt;/strong&gt; called &lt;a href=&quot;https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/&quot;&gt;Machine Learning, Data Science and Deep Learning with Python&lt;/a&gt;. It was my first course on Udemy, taught by &lt;strong&gt;Frank Kane&lt;/strong&gt;, founder of &lt;strong&gt;Sundog Education&lt;/strong&gt; and I thought it was a very nice hands-on and practical course on &lt;strong&gt;machine learning&lt;/strong&gt; using &lt;strong&gt;Python&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;It attempts to &lt;strong&gt;cover as much ground&lt;/strong&gt; as possible, &lt;strong&gt;not going too deep&lt;/strong&gt; on each topic but the contents explained there were &lt;strong&gt;understandable&lt;/strong&gt; and it is a &lt;strong&gt;good way&lt;/strong&gt; to start and get an initial grasp of Machine Learning. I’ve previously worked with Machine Learning, so not all the concepts were new to me, but it was a very interesting experience nonetheless.&lt;/li&gt;
  &lt;li&gt;The course’s final project was writing code to run some algorithms on a mammographic masses dataset for classification purposes, that is to detect breast cancer, which was a simple way of implementing some of the things I learned in the course. The code is hosted &lt;a href=&quot;https://github.com/Marko50/udemy-machine-learning-data-science-deep-learning-python&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/posts/udemy-ml-ds-dl-python/certificate.png&quot; alt=&quot;Certificate&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="learning" /><category term="udemy" /><summary type="html">Recently, I finished an online course on Udemy called Machine Learning, Data Science and Deep Learning with Python. It was my first course on Udemy, taught by Frank Kane, founder of Sundog Education and I thought it was a very nice hands-on and practical course on machine learning using Python. It attempts to cover as much ground as possible, not going too deep on each topic but the contents explained there were understandable and it is a good way to start and get an initial grasp of Machine Learning. I’ve previously worked with Machine Learning, so not all the concepts were new to me, but it was a very interesting experience nonetheless. The course’s final project was writing code to run some algorithms on a mammographic masses dataset for classification purposes, that is to detect breast cancer, which was a simple way of implementing some of the things I learned in the course. The code is hosted here.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/udemy-ml-ds-dl-python.png" /><media:content medium="image" url="http://localhost:4000/udemy-ml-ds-dl-python.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Word2Vec and Elasticsearch for Recommendations</title><link href="http://localhost:4000/w2vec-cb-recommendations/" rel="alternate" type="text/html" title="Word2Vec and Elasticsearch for Recommendations" /><published>2021-06-26T00:00:00+01:00</published><updated>2021-06-26T00:00:00+01:00</updated><id>http://localhost:4000/word2vec-for-content-based-recommendations</id><content type="html" xml:base="http://localhost:4000/w2vec-cb-recommendations/">&lt;p&gt;&lt;strong&gt;What are content-based recommender systems?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Content-based recommender systems use item features and characteristics to produce similarities according to a similarity metric.
Then, for a certain target product, these systems recommend the most similar products using the previously computed similarities.
In a practical approach, if we consider the items as movies in the Netflix catalog, the features could be the product descriptions,
movie genre, cast, and producers. In this sense, we group movies with similar characteristics, that is with similar descriptions,
genres or actors who take part in that film.
A good example is if we think about the Star Wars movies. Since all the movies fall into the same category and have a similar cast and
descriptions, if we are viewing a page where we’re shown information about &lt;strong&gt;Episode IV – A New Hope&lt;/strong&gt;, a good recommendation of another
page would be &lt;strong&gt;Episode V – The Empire Strikes Back&lt;/strong&gt;, because these movies are closely related to each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How do we approach this mathematically?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Well, we use vectors, which can be called &lt;strong&gt;embedding vectors&lt;/strong&gt;. If somehow we could find a way to represent items as vectors in a certain &lt;strong&gt;vector space&lt;/strong&gt;, we could use numerous mathematical calculations
on these vectors to compute similarities. One of these possibilities is the &lt;strong&gt;cosine similarity&lt;/strong&gt;, which is a good way of providing a similarity measure
between non-binary(or categorical) vectors.
What this means is that, if we had two items, A and B, we could infer two vector representations of these items, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vec_A&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vec_B&lt;/code&gt; which would represent these items in a certain
vector space, with a certain length, we could compute the cosine similarity between them which would numerically express the similarity between A and B.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the snippet below we’re creating two random &lt;strong&gt;NumPy&lt;/strong&gt; vectors of a fixed length of 128 units between -1 and 1, using a uniform distribution function.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;vec_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vec_B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;What is the Cosine Similarity?&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As previously said, cosine similarity measures the similarity between two vectors. It does so, by computing the cosine of the angle between the two vectors, which means that
higher angles between two vectors representing two items will have a lower cosine value, than lower angles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/posts/word2vec/cosine_function.png&quot; alt=&quot;Cosine Function&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The calculation is done by dividing the &lt;strong&gt;dot product&lt;/strong&gt; of the two vectors by the multiplication of their &lt;strong&gt;euclidean norms&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/posts/word2vec/cosine_similarity.png&quot; alt=&quot;Cosine Function&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the snippet below we’re using sklearn.metrics.pairwise module which contains the cosine similarity function to calculate the cosine similarity between the
two previously created vectors.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec_B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How do we obtain these vectors?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, we need to consider this article is related to &lt;strong&gt;Natural Language Processing&lt;/strong&gt;. Taking this into consideration, there are multiple ways of responding to this question. We need to consider that standard &lt;strong&gt;integer encodings&lt;/strong&gt; or &lt;strong&gt;one-hot-encoding&lt;/strong&gt; representations do not help capture the semantic meaning of words in a text, so we need to think of other ways to do this.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More recent publications include &lt;strong&gt;Seq2Vec&lt;/strong&gt;, &lt;strong&gt;Doc2Vec&lt;/strong&gt;, or even &lt;strong&gt;Amazon’s BlazingText&lt;/strong&gt; which are all deep learning strategies, but in this post, I’m going to talk about &lt;strong&gt;Word2Vec&lt;/strong&gt; and how we can use Word2Vec to produce document embedding vectors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What is Word2Vec?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;You can find the original publication &lt;a href=&quot;https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf&quot;&gt;here&lt;/a&gt;.
We can refer to the &lt;strong&gt;TensorFlow&lt;/strong&gt; &lt;a href=&quot;https://www.tensorflow.org/tutorials/text/word2vec&quot;&gt;implementation&lt;/a&gt; which implements some ideas of the &lt;strong&gt;paper&lt;/strong&gt; above. There is also this very interesting &lt;a href=&quot;https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html&quot;&gt;article&lt;/a&gt; on &lt;strong&gt;KDNuggets&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Word2Vec is a recurrent neural network architecture that attempts to learn embedding vectors for words in a text corpus. In the previous TensorFlow guide they explain the Skip-Gram model implementation but there also exists the Continuous bag-of-Words model.&lt;/li&gt;
  &lt;li&gt;Often these datasets contain millions of words and standard feed forward-neural networks applied a full softmax function over the vocabulary, as an output layer, which was highly inefficient. The paper, offers multiple ways of solving this issue, either through &lt;strong&gt;Hierarchical Softmax (using Huffman Trees)&lt;/strong&gt; or using &lt;strong&gt;Noise Contrastive Estimation (NCE)&lt;/strong&gt;.
Here, we’ll approach NCE. NCE solves this issue, by differentiating data from noise. In reality, since we’re only interested in choosing noise words for not to calculate network weights, what we’re doing is called &lt;strong&gt;Negative Sampling&lt;/strong&gt;. For every word, called context word we choose a few negative sampling words, that do not appear together in a certain window (group of words) and choose not to calculate the weights for these words.&lt;/li&gt;
  &lt;li&gt;We can also use word sub-sampling which we can do to filter the most frequent words in the dataset corpus.&lt;/li&gt;
  &lt;li&gt;The input data is vectorized in one-hot-encodings, but the technique talked about in the previous point is used to improve efficiency.&lt;/li&gt;
  &lt;li&gt;There is a projection layer to reduce the dimensionality of the encoding vectors to a fixed vector length.&lt;/li&gt;
  &lt;li&gt;After we train a Word2Vec model, we can get predictions of embedding vectors for words in an item description.&lt;/li&gt;
  &lt;li&gt;To get item vectors we could, for instance, average the embedding vectors for each word in a description.&lt;/li&gt;
  &lt;li&gt;You could train this neural network using cloud services like &lt;strong&gt;AWS Sagemaker&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Where does Elasticseach fit in?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Well if we manage to obtain a working way of obtaining item vectors, we need to take into account that similarity comparison is no easy task. After obtaining the item vectors we need to calculate similarities for every pair of items we know to exist. We can construct a similarity matrix of items, where we are only interested in half of the matrix since the similarity of items A and B is the same as items B and A. After this, we need to make a search for each item, for the highest K similarities so that we can see which items are supposed to be recommended. This computational process has &lt;strong&gt;O(n²) complexity&lt;/strong&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; is the number of items. This is similar to the &lt;strong&gt;nearest neighbor search&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;One of the options is to store and perform this search in an Elasticsearch cluster, that can act as a database. According to this &lt;a href=&quot;https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch&quot;&gt;article&lt;/a&gt;, we could create an Elasticsearch index in a cluster whose mappings contain a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dense_vector&lt;/code&gt; related to a certain field identified by a name.&lt;/li&gt;
  &lt;li&gt;Elasticsearch provides the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cosineSimilarity&lt;/code&gt; function for querying purposes, where it returns the items more similar to the one provided in the parameters.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="recommendations" /><category term="machine-learning" /><category term="content-based" /><category term="NLP" /><summary type="html">What are content-based recommender systems?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/word2vec.png" /><media:content medium="image" url="http://localhost:4000/word2vec.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Online course about Machine Learning with Python</title><link href="http://localhost:4000/udemy-ml-ds-dl-python/" rel="alternate" type="text/html" title="Online course about Machine Learning with Python" /><published>2021-06-26T00:00:00+01:00</published><updated>2021-06-26T00:00:00+01:00</updated><id>http://localhost:4000/udemy-ml-dl-python</id><content type="html" xml:base="http://localhost:4000/udemy-ml-ds-dl-python/">&lt;ul&gt;
  &lt;li&gt;Recently, I finished an online course on &lt;strong&gt;Udemy&lt;/strong&gt; called &lt;a href=&quot;https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/&quot;&gt;Machine Learning, Data Science and Deep Learning with Python&lt;/a&gt;. It was my first course on Udemy, taught by &lt;strong&gt;Frank Kane&lt;/strong&gt;, founder of &lt;strong&gt;Sundog Education&lt;/strong&gt; and I thought it was a very nice hands-on and practical course on &lt;strong&gt;machine learning&lt;/strong&gt; using &lt;strong&gt;Python&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;It attempts to &lt;strong&gt;cover as much ground&lt;/strong&gt; as possible, &lt;strong&gt;not going too deep&lt;/strong&gt; on each topic but the contents explained there were &lt;strong&gt;understandable&lt;/strong&gt; and it is a &lt;strong&gt;good way&lt;/strong&gt; to start and get an initial grasp of Machine Learning. I’ve previously worked with Machine Learning, so not all the concepts were new to me, but it was a very interesting experience nonetheless.&lt;/li&gt;
  &lt;li&gt;The course’s final project was writing code to run some algorithms on a mammographic masses dataset for classification purposes, that is to detect breast cancer, which was a simple way of implementing some of the things I learned in the course. The code is hosted &lt;a href=&quot;https://github.com/Marko50/udemy-machine-learning-data-science-deep-learning-python&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/posts/udemy-ml-ds-dl-python/certificate.png&quot; alt=&quot;Certificate&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="docs" /><category term="learning" /><category term="udemy" /><summary type="html">Recently, I finished an online course on Udemy called Machine Learning, Data Science and Deep Learning with Python. It was my first course on Udemy, taught by Frank Kane, founder of Sundog Education and I thought it was a very nice hands-on and practical course on machine learning using Python. It attempts to cover as much ground as possible, not going too deep on each topic but the contents explained there were understandable and it is a good way to start and get an initial grasp of Machine Learning. I’ve previously worked with Machine Learning, so not all the concepts were new to me, but it was a very interesting experience nonetheless. The course’s final project was writing code to run some algorithms on a mammographic masses dataset for classification purposes, that is to detect breast cancer, which was a simple way of implementing some of the things I learned in the course. The code is hosted here.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/udemy-ml-ds-dl-python.png" /><media:content medium="image" url="http://localhost:4000/udemy-ml-ds-dl-python.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Word2Vec and Elasticsearch for Recommendations</title><link href="http://localhost:4000/w2vec-cb-recommendations/" rel="alternate" type="text/html" title="Word2Vec and Elasticsearch for Recommendations" /><published>2021-06-26T00:00:00+01:00</published><updated>2021-06-26T00:00:00+01:00</updated><id>http://localhost:4000/word2vec-for-content-based-recommendations</id><content type="html" xml:base="http://localhost:4000/w2vec-cb-recommendations/">&lt;p&gt;&lt;strong&gt;What are content-based recommender systems?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Content-based recommender systems use item features and characteristics to produce similarities according to a similarity metric.
Then, for a certain target product, these systems recommend the most similar products using the previously computed similarities.
In a practical approach, if we consider the items as movies in the Netflix catalog, the features could be the product descriptions,
movie genre, cast, and producers. In this sense, we group movies with similar characteristics, that is with similar descriptions,
genres or actors who take part in that film.
A good example is if we think about the Star Wars movies. Since all the movies fall into the same category and have a similar cast and
descriptions, if we are viewing a page where we’re shown information about &lt;strong&gt;Episode IV – A New Hope&lt;/strong&gt;, a good recommendation of another
page would be &lt;strong&gt;Episode V – The Empire Strikes Back&lt;/strong&gt;, because these movies are closely related to each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How do we approach this mathematically?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Well, we use vectors, which can be called &lt;strong&gt;embedding vectors&lt;/strong&gt;. If somehow we could find a way to represent items as vectors in a certain &lt;strong&gt;vector space&lt;/strong&gt;, we could use numerous mathematical calculations
on these vectors to compute similarities. One of these possibilities is the &lt;strong&gt;cosine similarity&lt;/strong&gt;, which is a good way of providing a similarity measure
between non-binary(or categorical) vectors.
What this means is that, if we had two items, A and B, we could infer two vector representations of these items, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vec_A&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vec_B&lt;/code&gt; which would represent these items in a certain
vector space, with a certain length, we could compute the cosine similarity between them which would numerically express the similarity between A and B.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the snippet below we’re creating two random &lt;strong&gt;NumPy&lt;/strong&gt; vectors of a fixed length of 128 units between -1 and 1, using a uniform distribution function.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;vec_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vec_B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;What is the Cosine Similarity?&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As previously said, cosine similarity measures the similarity between two vectors. It does so, by computing the cosine of the angle between the two vectors, which means that
higher angles between two vectors representing two items will have a lower cosine value, than lower angles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/posts/word2vec/cosine_function.png&quot; alt=&quot;Cosine Function&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The calculation is done by dividing the &lt;strong&gt;dot product&lt;/strong&gt; of the two vectors by the multiplication of their &lt;strong&gt;euclidean norms&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/posts/word2vec/cosine_similarity.png&quot; alt=&quot;Cosine Function&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the snippet below we’re using sklearn.metrics.pairwise module which contains the cosine similarity function to calculate the cosine similarity between the
two previously created vectors.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec_A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec_B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;How do we obtain these vectors?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, we need to consider this article is related to &lt;strong&gt;Natural Language Processing&lt;/strong&gt;. Taking this into consideration, there are multiple ways of responding to this question. We need to consider that standard &lt;strong&gt;integer encodings&lt;/strong&gt; or &lt;strong&gt;one-hot-encoding&lt;/strong&gt; representations do not help capture the semantic meaning of words in a text, so we need to think of other ways to do this.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More recent publications include &lt;strong&gt;Seq2Vec&lt;/strong&gt;, &lt;strong&gt;Doc2Vec&lt;/strong&gt;, or even &lt;strong&gt;Amazon’s BlazingText&lt;/strong&gt; which are all deep learning strategies, but in this post, I’m going to talk about &lt;strong&gt;Word2Vec&lt;/strong&gt; and how we can use Word2Vec to produce document embedding vectors.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;What is Word2Vec?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;You can find the original publication &lt;a href=&quot;https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf&quot;&gt;here&lt;/a&gt;.
We can refer to the &lt;strong&gt;TensorFlow&lt;/strong&gt; &lt;a href=&quot;https://www.tensorflow.org/tutorials/text/word2vec&quot;&gt;implementation&lt;/a&gt; which implements some ideas of the &lt;strong&gt;paper&lt;/strong&gt; above. There is also this very interesting &lt;a href=&quot;https://www.kdnuggets.com/2019/07/introduction-noise-contrastive-estimation.html&quot;&gt;article&lt;/a&gt; on &lt;strong&gt;KDNuggets&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Word2Vec is a recurrent neural network architecture that attempts to learn embedding vectors for words in a text corpus. In the previous TensorFlow guide they explain the Skip-Gram model implementation but there also exists the Continuous bag-of-Words model.&lt;/li&gt;
  &lt;li&gt;Often these datasets contain millions of words and standard feed forward-neural networks applied a full softmax function over the vocabulary, as an output layer, which was highly inefficient. The paper, offers multiple ways of solving this issue, either through &lt;strong&gt;Hierarchical Softmax (using Huffman Trees)&lt;/strong&gt; or using &lt;strong&gt;Noise Contrastive Estimation (NCE)&lt;/strong&gt;.
Here, we’ll approach NCE. NCE solves this issue, by differentiating data from noise. In reality, since we’re only interested in choosing noise words for not to calculate network weights, what we’re doing is called &lt;strong&gt;Negative Sampling&lt;/strong&gt;. For every word, called context word we choose a few negative sampling words, that do not appear together in a certain window (group of words) and choose not to calculate the weights for these words.&lt;/li&gt;
  &lt;li&gt;We can also use word sub-sampling which we can do to filter the most frequent words in the dataset corpus.&lt;/li&gt;
  &lt;li&gt;The input data is vectorized in one-hot-encodings, but the technique talked about in the previous point is used to improve efficiency.&lt;/li&gt;
  &lt;li&gt;There is a projection layer to reduce the dimensionality of the encoding vectors to a fixed vector length.&lt;/li&gt;
  &lt;li&gt;After we train a Word2Vec model, we can get predictions of embedding vectors for words in an item description.&lt;/li&gt;
  &lt;li&gt;To get item vectors we could, for instance, average the embedding vectors for each word in a description.&lt;/li&gt;
  &lt;li&gt;You could train this neural network using cloud services like &lt;strong&gt;AWS Sagemaker&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Where does Elasticseach fit in?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Well if we manage to obtain a working way of obtaining item vectors, we need to take into account that similarity comparison is no easy task. After obtaining the item vectors we need to calculate similarities for every pair of items we know to exist. We can construct a similarity matrix of items, where we are only interested in half of the matrix since the similarity of items A and B is the same as items B and A. After this, we need to make a search for each item, for the highest K similarities so that we can see which items are supposed to be recommended. This computational process has &lt;strong&gt;O(n²) complexity&lt;/strong&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; is the number of items. This is similar to the &lt;strong&gt;nearest neighbor search&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;One of the options is to store and perform this search in an Elasticsearch cluster, that can act as a database. According to this &lt;a href=&quot;https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch&quot;&gt;article&lt;/a&gt;, we could create an Elasticsearch index in a cluster whose mappings contain a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dense_vector&lt;/code&gt; related to a certain field identified by a name.&lt;/li&gt;
  &lt;li&gt;Elasticsearch provides the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cosineSimilarity&lt;/code&gt; function for querying purposes, where it returns the items more similar to the one provided in the parameters.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="docs" /><category term="recommendations" /><category term="machine-learning" /><category term="content-based" /><category term="NLP" /><summary type="html">What are content-based recommender systems?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/word2vec.png" /><media:content medium="image" url="http://localhost:4000/word2vec.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>